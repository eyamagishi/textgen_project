# main.py

import yaml
from llama_cpp import Llama
from rich.console import Console
from rich.table import Table
from rich.progress import track
from pathlib import Path
import time
import sys

# === åˆæœŸåŒ– ===
console = Console()
CONFIG_PATH = Path("config.yaml")
PROMPT_PATH = Path("prompts/story.txt")
OUTPUT_PATH = Path("output.txt")

def load_config(path: Path) -> dict:
    """
    æŒ‡å®šã•ã‚ŒãŸYAMLãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚

    å¼•æ•°:
        path (Path): è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹

    æˆ»ã‚Šå€¤:
        dict: èª­ã¿è¾¼ã¾ã‚ŒãŸè¨­å®šã®è¾æ›¸

    ä¾‹å¤–:
        SystemExit: ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã‚„èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ãŸå ´åˆ
    """
    if not path.exists():
        console.print(f"[red]âŒ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {path}[/red]")
        sys.exit(1)

    with path.open("r", encoding="utf-8") as f:
        return yaml.safe_load(f)

def display_config(config: dict) -> None:
    """
    è¨­å®šå†…å®¹ã‚’ãƒ†ãƒ¼ãƒ–ãƒ«å½¢å¼ã§ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«è¡¨ç¤ºã—ã¾ã™ã€‚

    å¼•æ•°:
        config (dict): è¡¨ç¤ºã™ã‚‹è¨­å®šã®è¾æ›¸
    """
    table = Table(title="ğŸ› ï¸ ãƒ¢ãƒ‡ãƒ«è¨­å®š")
    table.add_column("ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿", style="cyan", no_wrap=True)
    table.add_column("å€¤", style="magenta")
    for key, value in config.items():
        table.add_row(str(key), str(value))
    console.print(table)

def load_prompt(path: Path) -> str:
    """
    æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚

    å¼•æ•°:
        path (Path): ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹

    æˆ»ã‚Šå€¤:
        str: èª­ã¿è¾¼ã¾ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ–‡å­—åˆ—

    ä¾‹å¤–:
        SystemExit: ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆ
    """
    if not path.exists():
        console.print(f"[red]âŒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {path}[/red]")
        sys.exit(1)

    return path.read_text(encoding="utf-8").strip()

def initialize_model(config: dict) -> Llama:
    """
    è¨­å®šã«åŸºã¥ã„ã¦ Llama ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ã—ã¾ã™ã€‚

    å¼•æ•°:
        config (dict): ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ã«ä½¿ç”¨ã™ã‚‹è¨­å®šè¾æ›¸

    æˆ»ã‚Šå€¤:
        Llama: åˆæœŸåŒ–ã•ã‚ŒãŸ Llama ãƒ¢ãƒ‡ãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹

    ä¾‹å¤–:
        SystemExit: ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ãŸå ´åˆ
    """
    console.print("ğŸ“¦ ãƒ¢ãƒ‡ãƒ«ã‚’æº–å‚™ä¸­...", style="yellow")
    for _ in track(range(10), description="ğŸ”§ ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ä¸­..."):
        time.sleep(0.02)

    try:
        return Llama(
            model_path=config["model_path"],
            n_ctx=config.get("context_length", 512),
            n_threads=config.get("threads", 2),
            verbose=False
        )
    except Exception as e:
        console.print(f"[red]âŒ ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}[/red]")
        sys.exit(1)

def generate_response(llm: Llama, prompt: str, config: dict) -> str:
    """
    ãƒ¢ãƒ‡ãƒ«ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä¸ãˆã¦å¿œç­”ã‚’ç”Ÿæˆã—ã¾ã™ã€‚

    å¼•æ•°:
        llm (Llama): åˆæœŸåŒ–æ¸ˆã¿ã® Llama ãƒ¢ãƒ‡ãƒ«
        prompt (str): å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        config (dict): ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å«ã‚€è¨­å®šè¾æ›¸

    æˆ»ã‚Šå€¤:
        str: ç”Ÿæˆã•ã‚ŒãŸå¿œç­”ãƒ†ã‚­ã‚¹ãƒˆ

    ä¾‹å¤–:
        SystemExit: å¿œç­”ç”Ÿæˆã«å¤±æ•—ã—ãŸå ´åˆ
    """
    console.print("ğŸ§  å¿œç­”ã‚’ç”Ÿæˆä¸­...", style="cyan")
    try:
        output = llm(
            prompt,
            max_tokens=config.get("max_tokens", 32),
            temperature=config.get("temperature", 0.7),
            top_k=config.get("top_k", 40),
            stop=["</s>"]
        )
        return output["choices"][0]["text"].strip()
    except Exception as e:
        console.print(f"[red]âŒ å¿œç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}[/red]")
        sys.exit(1)

def save_output(text: str, path: Path) -> None:
    """
    ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã™ã€‚

    å¼•æ•°:
        text (str): ä¿å­˜ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ
        path (Path): ä¿å­˜å…ˆã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    """
    try:
        path.write_text(text, encoding="utf-8")
        console.print(f"ğŸ’¾ å‡ºåŠ›ã‚’ä¿å­˜ã—ã¾ã—ãŸ: [green]{path}[/green]")
    except Exception as e:
        console.print(f"[yellow]âš ï¸ å‡ºåŠ›ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}[/yellow]")

def main():
    """
    ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ãƒ¡ã‚¤ãƒ³å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
    """
    config = load_config(CONFIG_PATH)
    display_config(config)

    prompt = load_prompt(PROMPT_PATH)
    console.rule("[bold cyan]ğŸ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ")
    console.print(prompt, style="white on black")

    llm = initialize_model(config)
    response = generate_response(llm, prompt, config)

    console.rule("[bold green]ğŸ“¤ ç”Ÿæˆçµæœ")
    console.print(response, style="bold white")

    save_output(response, OUTPUT_PATH)


if __name__ == "__main__":
    main()
