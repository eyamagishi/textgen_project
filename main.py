# main.py

import yaml
from llama_cpp import Llama
from utils.tokenizer import load_prompt

# è¨­å®šèª­ã¿è¾¼ã¿
with open("config.yaml", "r", encoding="utf-8") as f:
    config = yaml.safe_load(f)

# ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
llm = Llama(
    model_path=config["model_path"],
    n_ctx=config["context_length"],
    n_threads=config["threads"]
)

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆèª­ã¿è¾¼ã¿
prompt = load_prompt("prompts/story.txt")

# ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
print("ğŸ§  ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†ã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é€ä¿¡ä¸­...")
output = llm(
    prompt,
    max_tokens=config["max_tokens"],
    temperature=config["temperature"],
    top_k=config["top_k"]
)
print("âœ… å¿œç­”ç”Ÿæˆå®Œäº†")
print(output["choices"][0]["text"])

print("ğŸ“ ç”Ÿæˆçµæœ:\n", output["choices"][0]["text"])
