# core/model_runner.py

# === æ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª ===
import time

# === ã‚µãƒ¼ãƒ‰ãƒ‘ãƒ¼ãƒ†ã‚£ãƒ©ã‚¤ãƒ–ãƒ©ãƒª ===
from rich.console import Console
from rich.progress import track
from llama_cpp import Llama

# === ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« ===
# ï¼ˆã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã§ã¯ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ä½¿ç”¨ã—ã¦ã„ã¾ã›ã‚“ï¼‰

console = Console()

def initialize_model(config: dict) -> Llama:
    """
    Llamaãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ã—ã¾ã™ã€‚
    """
    console.print("ğŸ“¦ ãƒ¢ãƒ‡ãƒ«ã‚’æº–å‚™ä¸­...", style="yellow")
    for _ in track(range(10), description="ğŸ”§ ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ä¸­..."):
        time.sleep(0.02)

    return Llama(
        model_path=config["model_path"],
        n_ctx=config.get("context_length", 512),
        n_threads=config.get("threads", 2),
        verbose=False
    )

def generate_response(llm: Llama, prompt: str, config: dict) -> str:
    """
    ãƒ¢ãƒ‡ãƒ«ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä¸ãˆã¦å¿œç­”ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
    """
    console.print("ğŸ§  å¿œç­”ã‚’ç”Ÿæˆä¸­...", style="cyan")
    output = llm(
        prompt,
        max_tokens=config.get("max_tokens", 32),
        temperature=config.get("temperature", 0.7),
        top_k=config.get("top_k", 40),
        stop=["</s>"]
    )
    return output["choices"][0]["text"].strip()
